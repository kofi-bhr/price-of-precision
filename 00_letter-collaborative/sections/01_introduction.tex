\section{Introduction}

In 2018, Amazon scrapped an artificial intelligence recruiting tool after discovering a major flaw: it disliked women \citet{Dastin2018}. The system penalized resumes containing the word ``women's'' and downgraded graduates from all-women's colleges. Despite Amazon's technical expertise and goal to find the best talent, attempts to debias the system proved futile. Amazon ultimately abandoned the project.

This exemplifies a persisting puzzle in the modern economy. Sophisticated firms with no discriminatory intent and access to vast individual-level data still produce biased outcomes. Traditional economic theories struggle to explain this. Taste-based theories \citet{Becker1957} assume personal animus, while statistical discrimination theories \citet{Phelps1972,Arrow1973} assume group identity serves as a proxy for missing information; neither fits today's data-oriented, algorithmic environment.

This paper proposes and formalizes a third mechanism: \textit{informational discrimination}. Our central premise is that there exists a technological frontier between fairness and accuracy \citet{Kleinberg2017,Chouldechova2017}. Efforts to reduce algorithmic bias often decrease predictive power. We model a firm that understands this trade-off and rationally chooses optimal bias to maximize hired worker productivity. The severity of this trade-off is governed by parameter $\kappa$.

Our main result shows that a profit-maximizing firm will rationally choose strictly positive bias ($b^* > 0$) even under ideal conditions: when protected groups are ex-ante identical in productivity and debiasing is technologically costless. The logic follows Holmstrom's Informativeness Principle \citet{Holmstrom1979}. The firm chooses between a ``fair'' signal ($b=0$), which treats groups identically but is noisy, and a ``biased'' signal ($b>0$), which systematically disadvantages one group but provides greater precision. The biased signal's superior informational content makes it privately optimal despite discriminatory outcomes.

The model yields a sharp comparative static: optimal bias $b^*$ increases in the trade-off severity $\kappa$. Industries with complex prediction tasks or imbalanced data should exhibit more bias, independent of discriminatory intent. We analyze welfare implications and show the privately optimal bias exceeds the social optimum, providing rationale for policy intervention.

This paper contributes to three literatures: economic discrimination by formalizing a novel information-based channel, the economics of AI by providing micro-foundations for algorithmic bias, and information design \citet{Kamenica2011,Bergemann2019} by modeling constrained information structure choice.
