\section{Introduction}

In 2018, Amazon scrapped an artificial intelligence recruiting tool after discovering a major flaw: it disliked women \citep{Dastin2018}. The system was designed to automate the search for top software developers by evaluating parsed resumes. Yet, the machine learning models taught themselves that male candidates were preferable, penalizing resumes that contained the word ``women's" (as in ``women's chess club captain") and downgrading graduates from two all-women's colleges. Amazon's engineers tried to amend the system to make it neutral, but they could not guarantee that the machine would not devise new, equally discriminatory ways of sorting candidates. Amazon ultimately abandoned the project.

This instance is representative of a persisting puzzle in the modern economy. Amazon is a sophisticated, profit-maximizing firm. It faced no shortage of technical expertise or computational resources, and its goal was to find the best possible talent, not to discriminate. Yet, its attempt to build a purely meritocratic algorithm resulted in a biased system that was ultimately irreparable. For years, similar issues of persistent, unintended bias have emerged in algorithms for credit scoring, medical diagnoses, and criminal justice risk assessment. These cases are difficult to explain with the canonical economic models of discrimination. Taste-based theories \citep{Becker1957} struggle because the principal is an algorithm, not a manager with personal animus, and competitive pressures should, in theory, punish such inefficiency. Statistical discrimination theories \citep{Phelps1972, Arrow1973} are also an imperfect fit, as they assume that group identity is used as a proxy due to a lack of individual-level data; today's algorithms, however, operate in a data-rich environment with access to hundreds of individual-specific features.

This paper proposes and formalizes a third mechanism for discrimination that is not about tastes or beliefs, but about the inherent technological trade-offs in prediction. We term this mechanism ``informational discrimination." Our central premise is that there exists a technological frontier between fairness and accuracy \citep{Kleinberg2017, Chouldechova2017}. Efforts to reduce an algorithm's disparate impact (debiasing) often come at the cost of reducing its predictive power. We model a firm that understands this trade-off and rationally chooses an optimal level of bias to maximize the productivity of its hired workers. The severity of this trade-off is governed by a single parameter, $\kappa$, which represents the steepness of the fairness-accuracy frontier.

Our main theoretical result shows that a profit-maximizing firm will rationally choose a strictly positive level of bias ($b^* > 0$) even under the cleanest possible conditions: when protected groups are ex-ante identical in average productivity and debiasing is technologically costless. The economic logic for this result follows from Holmstrom's Informativeness Principle \citep{Holmstrom1979} . The firm faces a choice between a perfectly ``fair'' signal ($b=0$), which treats all groups identically but is noisy, and a ``biased'' signal ($b>0$), which systematically disadvantages one group but provides greater overall precision. Even though the biased signal creates discriminatory outcomes, its superior informational content makes it privately optimal for the firm. The marginal gain in predictive accuracy from accepting a small amount of bias outweighs the marginal cost of the signal's distortion.

The model yields a sharp comparative static: the firm's optimal level of bias, $b^*$, is increasing in the severity of the underlying technological trade-off, $\kappa$. This provides a set of testable predictions about where and why we should observe algorithmic bias. Industries with more complex prediction tasks or less balanced data (higher $\kappa$) should exhibit more bias, independent of discriminatory intent. We then analyze the welfare implications of this mechanism. The firm's privately optimal choice, $b^*$, is socially inefficient, creating a deadweight loss by systematically disadvantaging one group. We show that the socially optimal level of bias, $b^{**}$, is strictly less than what the firm chooses. This divergence provides a clear rationale for policy intervention. Our framework suggests that simple mandates (e.g., forcing $b=0$) are blunt and inefficient instruments because they ignore the underlying technological constraint. More nuanced policies, such as Pigouvian taxes that force the firm to internalize the social cost of bias, or R\&D subsidies that fund the development of better technology to ``flatten'' the frontier (lower $\kappa$), are more effective at aligning private incentives with social goals.

This paper contributes to three strands of literature. First, we contribute to the foundational literature on economic discrimination by formalizing a novel, information-based channel that is distinct from the canonical works of Becker, Phelps, and Arrow. Second, we contribute to the emerging field of the ``economics of AI" by providing a tractable micro-foundation for the fairness-accuracy trade-off and exploring the strategic incentives it creates for firms. Third, our work connects to the literature on information design \citep{Kamenica2011, Bergemann2019} by modeling a principal who designs an information structure for her own use, constrained by a technological frontier that links the signal's informativeness to its disparate impact. By isolating this informational mechanism, our model provides a new theoretical lens for understanding a critical policy challenge and offers a framework for designing more effective interventions in an increasingly automated world.