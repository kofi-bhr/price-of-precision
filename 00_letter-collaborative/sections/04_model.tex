\section{Model}

A risk-neutral firm hires candidates with productivity $\theta \sim N(\mu, \sigma_\theta^2)$ from groups $g \in \{0, 1\}$. We make three key assumptions to isolate the informational mechanism.

\textbf{Assumption 1 (Identical Productivity, Differential Measurement):} True productivity distributions are identical across groups ($\mathbb{E}[\theta|g=1] = \mathbb{E}[\theta|g=0] = \mu$), but the firm observes a noisy signal $s_g$ that may have group-specific measurement error. This isolates cases where group membership is informative about measurement quality, not ability.

\textbf{Assumption 2 (Observable Groups):} The firm observes group membership $g$, allowing us to focus on the pure informational channel rather than statistical inference problems.

\textbf{Assumption 3 (Signal Structure):} The firm observes signal $s_g = \theta + \eta_g + b \cdot g + \varepsilon(b)$, where $\eta_g \sim N(0, \sigma_{\eta,g}^2)$ is exogenous group-specific noise, $b$ is the firm's bias choice, and $\varepsilon(b) \sim N(0, \sigma_\varepsilon^2(b))$ represents algorithmic noise that depends on bias level.

The model's core mechanism is a trade-off between signal precision and bias. Fairness constraints in machine learning act as regularizers, increasing estimator variance \citet{Kamishima2012,Wick2019}. This creates a fundamental tension: algorithms can be made fairer, but only at the cost of reduced accuracy. We formalize this relationship as:

\begin{equation}
\sigma_\varepsilon^2(b) = \sigma_0^2 + \kappa(b_{max} - b) \quad \text{for } b \in [0, b_{max}]
\end{equation}

where $b=b_{max}$ represents the most precise but most biased signal, $b=0$ represents a fair but noisy signal, and $\kappa > 0$ captures the steepness of this trade-off.

The firm chooses bias $b$ and hiring threshold $t$ to maximize expected productivity of hired workers:
\begin{equation}
\max_{b, t} \ \mathbb{E}[U(b,t)] = \sum_{g \in \{0,1\}} \pi_g \int_t^\infty \mathbb{E}[\theta | s, g, b] f(s|g, b) ds
\end{equation}

Under our assumptions, the observed signals follow $s_0 \sim N(\mu, \sigma_s^2(b))$ and $s_1 \sim N(\mu + b, \sigma_s^2(b))$, where $\sigma_s^2(b) = \sigma_\theta^2 + \sigma_\varepsilon^2(b)$. Using Bayesian updating, the posterior expectation is:
\begin{equation}
\mathbb{E}[\theta | s, g, b] = \frac{\sigma_\theta^2 (s - b \cdot \mathbf{1}_{g=1}) + \sigma_\varepsilon^2(b) \mu}{\sigma_\theta^2 + \sigma_\varepsilon^2(b)}
\end{equation}
