\appendix
\section{Mathematical Appendix}

\textbf{Model Setup:} For candidate from group $g \in \{0,1\}$ with productivity $\theta \sim N(\mu, \sigma_\theta^2)$, the firm observes signal $s_g = \theta + b \cdot \mathbf{1}_{g=1} + \varepsilon(b)$ where $\varepsilon(b) \sim N(0, \sigma_\varepsilon^2(b))$ and $\sigma_\varepsilon^2(b) = \sigma_0^2 + \kappa(b_{max} - b)$. Under our baseline assumption of identical group productivity, the signals follow:
\begin{align}
s_0 &\sim N(\mu, \sigma_s^2(b)) \\
s_1 &\sim N(\mu + b, \sigma_s^2(b))
\end{align}
where $\sigma_s^2(b) = \sigma_\theta^2 + \sigma_\varepsilon^2(b) = \sigma_\theta^2 + \sigma_0^2 + \kappa(b_{max} - b)$.

\textbf{Posterior Beliefs:} Using Bayesian updating, for signal $s$ from group $g$:
\begin{equation}
E[\theta | s, g, b] = \frac{\sigma_\theta^2 (s - b \cdot \mathbf{1}_{g=1}) + \sigma_\varepsilon^2(b) \mu}{\sigma_\theta^2 + \sigma_\varepsilon^2(b)} \label{eq:posterior}
\end{equation}
The posterior precision is $\tau_s(b) = \frac{1}{\sigma_\theta^2 + \sigma_\varepsilon^2(b)}$.

\textbf{Firm's Optimization:} The firm chooses bias $b$ and threshold $t$ to maximize expected productivity:
\begin{equation}
V(b,t) = \sum_{g \in \{0,1\}} \pi_g \int_t^\infty E[\theta | s, g, b] f(s|g,b) \, ds \label{eq:value_function}
\end{equation}
For any given $b$, the optimal threshold $t^*(b)$ satisfies $E[\theta | t^*(b), g, b] = \mu$.

\begin{lemma}[Threshold Symmetry]
At $b = 0$, the optimal threshold is $t^*(0) = \mu$.
\end{lemma}
\begin{proof}
At $b = 0$, both groups have identical signal distributions. The posterior expectation is:
\begin{equation}
E[\theta | t^*(0), g, 0] = \frac{\sigma_\theta^2 t^*(0) + \sigma_\varepsilon^2(0) \mu}{\sigma_\theta^2 + \sigma_\varepsilon^2(0)}
\end{equation}
Setting equal to the reservation value $\mu$ and solving yields $t^*(0) = \mu$.
\end{proof}

\begin{lemma}[Posterior Derivatives]
The partial derivatives of the posterior mean are:
\begin{align}
\frac{\partial E[\theta | s, g, b]}{\partial b} &= -\frac{\sigma_\theta^2 \mathbf{1}_{g=1}}{\sigma_s^2(b)} + \kappa \frac{E[\theta|s,g,b] - \mu}{\sigma_s^2(b)} \\
\frac{\partial E[\theta | s, g, b]}{\partial \sigma_\varepsilon^2} &= \frac{\sigma_\theta^2(\mu - s + b \cdot \mathbf{1}_{g=1})}{(\sigma_s^2(b))^2}
\end{align}
\end{lemma}

\begin{lemma}[Concavity of Value Function]
The firm's value function $V(b)$ is strictly concave: $\frac{d^2V}{db^2} < 0$.
\end{lemma}
\begin{proof}
Using the Envelope Theorem, the first derivative is:
\begin{equation}
V'(b) = \sum_{g \in \{0,1\}} \pi_g \int_{t^*(b)}^\infty \frac{\partial E[\theta|s,g,b]}{\partial b} f(s|g,b) \, ds
\end{equation}
Differentiating with Leibniz's rule and using the quadratic nature of the bias-precision tradeoff, both the threshold effect and intramarginal effect are negative. Therefore $V''(b) < 0$.
\end{proof}

\textbf{Proof of Proposition 1 (Existence of Optimal Bias):} We evaluate $\frac{dV}{db}$ at $b=0$. From the threshold symmetry lemma, $t^*(0)=\mu$, so we hire candidates with $s \geq \mu$.

The distortion effect for group 1 is:
\begin{equation}
\text{Distortion} = \pi_1 \, \mathbb{E}_{s_1}\left[-\frac{\sigma_\theta^2}{\sigma_s^2(0)} \mathbf{1}_{s_1 \geq \mu}\right] = -\frac{\pi_1 \sigma_\theta^2}{2\sigma_s^2(0)}
\end{equation}

The precision effect for both groups is:
\begin{equation}
\text{Precision} = \kappa \, \frac{\sigma_\theta^2}{(\sigma_s^2(0))^2} \, \mathbb{E}_{s,g}\left[(s-\mu)\mathbf{1}_{s \geq \mu}\right]
\end{equation}
For $N(\mu,\sigma_s^2(0))$, we have $\mathbb{E}[(s-\mu)\mathbf{1}_{s \geq \mu}] = \frac{\sigma_s(0)}{\sqrt{2\pi}}$, so:
\begin{equation}
\text{Precision} = \frac{\kappa \sigma_\theta^2}{\sigma_s^2(0)\sigma_s(0)\sqrt{2\pi}}
\end{equation}

The net effect at $b=0$ is:
\begin{equation}
\frac{dV}{db}\Big|_{b=0} = -\frac{\pi_1 \sigma_\theta^2}{2\sigma_s^2(0)} + \frac{\kappa \sigma_\theta^2}{\sigma_s^2(0)\sigma_s(0)\sqrt{2\pi}}
\end{equation}
This is positive when $\kappa \geq \pi_1 \sigma_s(0) \sqrt{\pi/2} \equiv \kappa_{\min}$. Combined with concavity, this implies $b^* > 0$ for $\kappa \geq \kappa_{\min}$. $\square$

\textbf{Proof of Proposition 2 (Comparative Static):} The optimal bias $b^*$ satisfies $\frac{dV(b^*)}{db} = 0$. By the Implicit Function Theorem:
\begin{equation}
\frac{\partial b^*}{\partial \kappa} = -\frac{\frac{\partial^2 V}{\partial b \partial \kappa}}{\frac{\partial^2 V}{\partial b^2}}\bigg|_{b=b^*}
\end{equation}
Since $V$ is concave, $\frac{\partial^2 V}{\partial b^2} < 0$. The cross-partial $\frac{\partial^2 V}{\partial b \partial \kappa} > 0$ because for hired candidates, the precision effect is positive. Therefore $\frac{\partial b^*}{\partial \kappa} > 0$. $\square$