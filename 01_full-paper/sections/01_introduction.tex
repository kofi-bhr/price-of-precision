\section{Introduction}

In 2018, Amazon scrapped an artificial intelligence recruiting tool after uncovering a major flaw: it disliked women \citep{Dastin2018}. The tool was designed to automate the search for top software engineers by looking at parsed resumes. Yet, the machine learning models taught themselves that male candidates were preferable, shirking resumes containing the word ``women's" (as in ``women's chess club captain") and downgrading graduates from all-women's colleges. Amazon's engineers tried to adjust the system to make it neutral, but they could not guarantee that the machine would not come up with new discriminatory ways of sorting candidates. Amazon ultimately ended up scrapping the project.

This was one instance of a problem that's becoming common quickly. Amazon is a sophisticated, profit-maximizing firm. It had no shortage of technical or computational power, and its goal was to find the best possible talent, not to discriminate. Yet, its try to build a purely meritocratic algorithm birthed a biased system that was ultimately unfixable. For years, eerily similar problems involving persistent, unintended bias have come up in algorithms for credit scoring, medical diagnoses, and criminal justice risk assessment, to name a few. These cases are hard to explain with the canonical economic models of discrimination. Taste-based theories \citep{Becker1957} struggle because the principal is an algorithm, not a manager with personal animus, and competitive pressures should, in theory, punish such inefficiency. Statistical discrimination theories \citep{Phelps1972, Arrow1973} are also an imperfect fit, because they assume that group identity is used as a proxy because of a lack of individual-level data; the algorithms we study, however, are data-rich and sometimes have access to even hundreds of individual-specific features.

We propose and formalize a third mechanism for discrimination about the inherent technological trade-offs in prediction. We call this mechanism ``informational discrimination." We offer that there exists a technological frontier between fairness and accuracy \citep{Kleinberg2017, Chouldechova2017}. Companies' attempts to ``fix" an algorithm's disparate impact (debiasing) often come at the cost of reducing its predictive ability. We thus model a firm that understands this trade-off and rationally chooses an optimal level of bias to maximize the productivity of its hired workers. The severity of this trade-off is governed by a single parameter, $\kappa$, representing the steepness of the frontier.

Our main theoretical result shows that a profit-maximizing firm will rationally choose a strictly positive level of bias ($b^* > 0$) even under the cleanest possible conditions: when protected groups are ex-ante identical in average productivity and debiasing is technologically costless. The economic logic for this result follows Holmstrom's Informativeness Principle \citep{Holmstrom1979}. The firm has a choice between a perfectly ``fair'' signal ($b=0$), which treats all groups identically but is noisy, and a ``biased'' signal ($b>0$), which often disadvantages one group but gives a greater overall prediction accuracy. Even though the biased signal leads to discrimination, its far better informational content makes it privately optimal for the firm. The marginal gain in accuracy from accepting a small amount of bias ends up outweighing the marginal cost of the signal's distortion.

The model shows a sharp comparative static: the firm's optimal level of bias, $b^*$, is increasing in the severity of the underlying technological trade-off, $\kappa$. This gives us testable predictions about where and why we should observe this algorithmic bias. Industries with more complicated prediction tasks or with far less balanced data (higher $\kappa$) should show more bias, independent of their discriminatory intent. We then examine the welfare implications of this mechanism. The firm's privately optimal choice, $b^*$, is socially painful, as it creates a deadweight loss by disadvantaging one group. We show that the socially optimal level of bias, $b^{**}$, is strictly less than what the firm chooses. This divergence provides a natural rationale for policy intervention. Our framework suggests that simple mandates (e.g., forcing $b=0$) are blunt and inefficient instruments because they ignore the underlying technological constraint. More nuanced policies, such as Pigouvian taxes that force the firm to internalize the social cost of their bias, or R\&D subsidies funding the development of better technology to ``flatten'' the frontier (lower $\kappa$), are far more effective at aligning private incentives with the firm's social goals.

This paper contributes to three areas of the literature. First, we contribute to the foundational literature on economic discrimination by formalizing an information-based channel, distinct from the canonical works of Becker, Phelps, and Arrow. Second, we contribute to the emerging field of the ``economics of AI" with a tractable micro-foundation for the fairness-accuracy trade-off and exploring the strategic incentives it creates for firms. Third, we connect to the literature on information design \citep{Kamenica2011, Bergemann2019} by modeling a principal who designs an information structure for her own use, constrained by a technological frontier that links the signal's informativeness to its disparate impact. In isolating this informational mechanism, our model generates a new theoretical lens for understanding an increasingly divisive policy issue and offers a framework to design better interventions moving forward.