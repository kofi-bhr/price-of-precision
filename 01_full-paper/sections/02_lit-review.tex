\section{Literature Review}

The fairness-accuracy trade-off is a recurring and increasingly central theme in the algorithmic fairness literature. Our work builds on a few major areas of this research.

The foundational challenge comes from the mathematical impossibility of simultaneously solving for multiple, maliciously intuitive-seeming fairness criteria. The work of \citet{Kleinberg2017} gives us a formal impossibility theorem, showing that for any non-trivial classifier, it is impossible to satisfy both calibration and equalized odds across groups with different base rates. This result is empirically corroborated by \citet{Chouldechova2017} in her analysis of the COMPAS recidivism prediction algorithm, showing that the algorithm could never be simultaneously calibrated and have equal false positive rates for different racial groups. These results together establish that trade-offs are a mathematical necessity.

Our paper bridges the computer science and economics perspectives on this problem. As \citet{Gans2025} notes, the economic approach focuses on welfare and incentives, while the computer science literature has traditionally focused on statistical fairness metrics. We follow the economic approach by modeling an explicit trade-off, but use the statistical findings from computer science to build micro-foundations for it. Work by \citet{Rambachan2020} works to create a micro-foundation for the economic method by decomposing some prediction differences, which aligns with our formalization of the trade-off parameter $\kappa$.

Finally, our conceptualization of the trade-off's magnitude comes from the ``fairness frontier'' framework by \citet{Liang2025}. They illustrate how the set of possible error rates over different groups creates a frontier, and the shape of this frontier directly dictates the severity of the trade-off. Here, we have a straightforward link between the statistical properties of the data and the resulting economic cost behind these fairness constraints, which is central to our model.

This work connects with an important literature in computer science showing how mathematically inefficient it is to satisfy multiple fairness criteria simultaneously. From this we have a rigorous foundation for why a trade-off between fairness and accuracy often exists. This section covers the central findings motivating our model's core trade-off.

\subsection{Foundational Theory}

The center of the fairness-accuracy trade-off is a set of theoretical results showing that it is mathematically impossible to satisfy multiple, intuitive fairness criteria simultaneously, except in some trivial cases.

\subsubsection{Kleinberg, Mullainathan, and Raghavan (2016)}

A paper by \citet{Kleinberg2017} sets forth an ``impossibility theorem'' of algorithmic fairness. It proves that for a scoring-based classifier, it is impossible to satisfy three main fairness conditions at the same time, unless the predictor is perfect or the base rates are equal across groups. Let $G$ be the protected attribute (group), $Y$ be the true outcome (e.g., $Y=1$ if a candidate succeeds), and $S$ be the algorithm's risk score. The three conflicting conditions are:
First, \textbf{calibration} requires that the score is an accurate reflection of risk, such that $Pr(Y=1 | S=s) = s$ for all score levels $s$. Second, \textbf{balance for the positive class} requires that the average score for those who are actually positive is the same across all groups: $E[S | Y=1, G=g_1] = E[S | Y=1, G=g_2]$. Finally, \textbf{balance for the negative class} requires that the average score for those who are actually negative is also the same across groups: $E[S | Y=0, G=g_1] = E[S | Y=0, G=g_2]$.
The paper proves that these three conditions can only hold simultaneously if base rates of the positive outcome are equal across groups ($Pr(Y=1 | G=g_1) = Pr(Y=1 | G=g_2)$) or if the prediction is perfect. Since base rates often differ in the real world, a decision-maker is forced to choose which fairness metric to violate. Thus, a rigorous foundation for why a trade-off exists.

\subsubsection{Chouldechova (2017)}

Independently and yet at the same time, \citet{Chouldechova2017} shows a similar impossibility result in the context of the COMPAS recidivism algorithm, again a direct conflict between predictive parity and error rate equality.

Reframing the problem, the argument is that one should use the most accurate algorithm possible and apply fairness considerations at the decision-making stage.

\subsubsection{Gans (2025)}

\citet{Gans2025} connects the two competing methods: the ``computer science'' option of directly regulating algorithms versus the ``economic'' one of regulating how to use them. The economic approach, articulated by \citet{Rambachan2020}, seeks to maximize a social welfare function by choosing an allocation rule $a(g,x)$ where $F(g,x)$ is the output of the most accurate possible prediction. Fairness comes in via group-specific welfare weights $\phi_g$ and decision thresholds $t*(g)$. This framework rhymes with our paper's setup, where the firm chooses the optimal point on a ``fairness-accuracy frontier.'' 

\subsubsection{Rambachan, Kleinberg, Ludwig, and Mullainathan (2020)}

This paper puts forth the micro-foundation for the economic approach. It decomposes the difference in average predictions between groups into three parts: (1) true base rate differences, (2) measurement error differences, and (3) estimation error differences. The computer science approach of forcing equal average predictions will often conflate these, while the economic approach argues for minimizing measurement and estimation error while handling true base rate differences with more explicit and group-specific decision thresholds. Our model offers a direct formalization of the trade-off arising from the second component of their decomposition: differences in measurement error. We explicitly model a scenario where a firm has to decide how to handle group-specific noise (our $\eta_g$ term), and show the ways the optimal response can lead to a biased but more precise signal. This connects their decomposition to a profit-maximizing incentive for disparate outcomes, even if true productivity distributions are identical.

\subsection{Visualizing the Trade-Off}

The ``fairness frontier'' offers us a useful geometric framework for interrogating the trade-off.

\subsubsection{Liang, Lu, Mu, and Okumura (2025)}

\citet{Liang2025} introduce the concept of a ``fairness frontier'' in the space of group-specific error rates ($e_0, e_1$). The frontier is the set of non-dominated algorithms. They show that the shape of this frontier depends on the statistical properties of the data. If the data is ``group-balanced,'' for example, the most ``fair'' algorithm (where $e_0 = e_1$) can be efficient (on the frontier). If the data is ``group-skewed,'' though, (i.e., more predictive for one group), the fairest point is inefficient (inside the frontier), creating a necessary trade-off. This framework is thus a direct micro-foundation for our parameter $\kappa$. A high $\kappa$ corresponds to a steep frontier (group-skewed data), where the marginal gain in precision from accepting some bias is large.
