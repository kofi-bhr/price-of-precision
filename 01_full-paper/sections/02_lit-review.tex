\section{Literature Review}

The fairness-accuracy trade-off is a central theme in the algorithmic fairness literature. Our work builds on several key pillars of this research.

The foundational challenge is rooted in the mathematical impossibility of simultaneously satisfying multiple, seemingly intuitive fairness criteria. The seminal work of \citet{Kleinberg2017} provides a formal impossibility theorem, demonstrating that for any non-trivial classifier, it is impossible to satisfy both calibration and equalized odds across groups with different base rates. This result was empirically corroborated by \citet{Chouldechova2017} in her analysis of the COMPAS recidivism prediction algorithm, which showed that the algorithm could not be simultaneously calibrated and have equal false positive rates for different racial groups. These impossibility results establish that trade-offs are not just an empirical observation but a mathematical necessity.

Our paper bridges the computer science and economics perspectives on this problem. As \citet{Gans2025} notes, the economic approach emphasizes welfare and incentives, while the computer science literature has traditionally focused on statistical fairness metrics. We follow the economic approach by modeling an explicit trade-off, but use the statistical findings from computer science to provide micro-foundations for it. Work by \citet{Rambachan2020} provides a micro-foundation for the economic approach by decomposing prediction differences, which aligns with our formalization of the trade-off parameter $\kappa$.

Finally, our conceptualization of the trade-off's magnitude is heavily influenced by the ``fairness frontier'' framework of \citet{Liang2025}. They illustrate how the set of achievable error rates across groups forms a frontier, and the shape of this frontier determines the severity of the trade-off. This provides a direct link between the statistical properties of the data and the economic cost of fairness constraints, which is central to our model.

This work connects with a foundational literature in computer science demonstrating the mathematical difficulty of satisfying multiple fairness criteria simultaneously. This provides a rigorous, axiomatic foundation for why a trade-off between fairness and accuracy often exists. This section reviews the key findings that motivate our model's core trade-off.

\subsection{Foundational Theory}

The cornerstone of the fairness-accuracy trade-off is a set of theoretical results demonstrating that it is mathematically impossible to satisfy multiple, intuitive fairness criteria simultaneously, except in trivial cases. This proves that a trade-off is not just an empirical observation but a fundamental mathematical constraint.

\subsubsection{Kleinberg, Mullainathan, and Raghavan (2016)}

A seminal paper by \citet{Kleinberg2017} provides an ``impossibility theorem'' of algorithmic fairness. It proves that for a scoring-based classifier, it is impossible to satisfy three key fairness conditions at the same time, unless the predictor is perfect or base rates are equal across groups. Let $G$ be the protected attribute (group), $Y$ be the true outcome (e.g., $Y=1$ if a candidate succeeds), and $S$ be the algorithm's risk score. The three conflicting conditions are:
First, \textbf{Calibration} requires that the score is an accurate reflection of risk, such that $Pr(Y=1 | S=s) = s$ for all score levels $s$. Second, \textbf{Balance for the Positive Class} requires that the average score for those who are actually positive is the same across all groups: $E[S | Y=1, G=g_1] = E[S | Y=1, G=g_2]$. Finally, \textbf{Balance for the Negative Class} requires that the average score for those who are actually negative is also the same across groups: $E[S | Y=0, G=g_1] = E[S | Y=0, G=g_2]$.
The paper proves that these three conditions can only hold simultaneously if base rates of the positive outcome are equal across groups ($Pr(Y=1 | G=g_1) = Pr(Y=1 | G=g_2)$) or if the prediction is perfect. Since base rates often differ in the real world, a decision-maker must choose which fairness metric to violate. This provides a rigorous foundation for why a trade-off exists.

\subsubsection{Chouldechova (2017)}

Independently and concurrently, \citet{Chouldechova2017} demonstrated a similar impossibility result in the context of the COMPAS recidivism algorithm, showing a direct conflict between predictive parity and error rate equality.

A recent and influential strand of literature from economics reframes the problem, arguing that one should use the most accurate algorithm possible and apply fairness considerations at the decision-making stage.

\subsubsection{Gans (2025)}

\citet{Gans2025} synthesizes the two competing approaches: the ``computer science'' approach of directly regulating algorithms versus the ``economic'' approach of regulating how algorithmic outputs are used. The economic approach, articulated by \citet{Rambachan2020}, aims to maximize a social welfare function by choosing an allocation rule $a(g,x)$ using the output of the most accurate possible prediction $F(g,x)$. Fairness is introduced via group-specific welfare weights $\phi_g$ and decision thresholds $t*(g)$. This framework aligns with our paper's setup, where the firm chooses the optimal point on a ``fairness-accuracy frontier.'' 

\subsubsection{Rambachan, Kleinberg, Ludwig, and Mullainathan (2020)}

This paper provides the micro-foundation for the economic approach. It decomposes the difference in average predictions between groups into three components: (1) true base rate differences, (2) differences in measurement error, and (3) differences in estimation error. The computer science approach of forcing equal average predictions can conflate these, while the economic approach argues for minimizing measurement and estimation error while handling true base rate differences through explicit, group-specific decision thresholds. Our model provides a direct formalization of the trade-off arising from the second component of their decomposition: differences in measurement error. We explicitly model a scenario where a firm must decide how to handle group-specific noise (our $\eta_g$ term), and show how the optimal response can lead to a biased but more precise signal. This links their decomposition directly to a profit-maximizing incentive for disparate outcomes, even when true productivity distributions are identical.

\subsection{Visualizing the Trade-Off}

The ``fairness frontier'' provides a powerful geometric framework for analyzing the trade-off.

\subsubsection{Liang, Lu, Mu, and Okumura (2025)}

\citet{Liang2025} introduce the concept of a ``fairness frontier'' in the space of group-specific error rates ($e_0, e_1$). The frontier is the set of non-dominated algorithms. They show that the shape of this frontier depends on the statistical properties of the data. If the data is ``group-balanced,'' the fairest algorithm (where $e_0 = e_1$) can be efficient (on the frontier). If the data is ``group-skewed'' (more predictive for one group), the fairest point is inefficient (inside the frontier), creating a necessary trade-off. This framework provides a direct micro-foundation for our parameter $\kappa$. A high $\kappa$ corresponds to a steep frontier (group-skewed data), where the marginal gain in precision from accepting some bias is large.
