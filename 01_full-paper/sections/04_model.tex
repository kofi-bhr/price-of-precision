\section{The Model}

\subsection{Primitives and assumptions}
A risk-neutral firm hires candidates. Candidate productivity $\theta$ draws from a Normal distribution, $\theta \sim N(\mu, \sigma_\theta^2)$. Candidates belong to group $g \in \{0, 1\}$.

\begin{assumption}[A1]
\textbf{Identical true productivity, differential measurement.} To isolate the informational mechanism, our baseline model assumes that the distribution of true underlying productivity, $\theta$, is identical across groups: $\mathbb{E}[\theta|g=1] = \mathbb{E}[\theta|g=0] = \mu$. However, the firm does not observe $\theta$ directly. Instead, it observes a signal $s$ that measures productivity with group-specific error. This sets up a scenario where group membership is informative solely about the nature of the measurement error. This case is policy-relevant in contexts where, for historical or structural reasons, data for one group is less reliable or more ``noisy" than for another.
\end{assumption}

\begin{assumption}[A2]
\textbf{Observable group membership.} The firm observes group membership $g$. While this may cause legal troubles in practice, it helps to isolate the informational mechanism. Our results extend to cases where group membership is imperfectly inferred from observable proxies, like the "Women's Chess Club Captain" example from the introduction, or first names associated with a gender, and last names associated with a race.
\end{assumption}

\begin{assumption}[A3]
\textbf{Signal and measurement error structure.} The firm observes a signal $s_g$ for a candidate from group $g$. The signal is a function of true productivity $\theta$, a group-specific measurement error $\eta_g$, and the firm's choice of bias $b$. We model the signal as $s_g = \theta + \eta_g + b \cdot g + \varepsilon(b)$. The term $\eta_g \sim N(0, \sigma_{\eta,g}^2)$ represents exogenous and group-specific noise. The firm's choice of $b$ can be interpreted as an adjustment to counteract suspected measurement error, which consequently affects the variance of the overall signal noise, $\varepsilon(b)$.
\end{assumption}

Given these assumptions, the signal $s_g$ for a candidate from group $g$ is normally distributed. Specifically, the signal distributions are:
\begin{align*}
s_0 &\sim \mathcal{N}(\mu, \sigma_\theta^2 + \sigma_\varepsilon^2(b)) \\
s_1 &\sim \mathcal{N}(\mu + b, \sigma_\theta^2 + \sigma_\varepsilon^2(b))
\end{align*}
We denote the conditional probability density function of the signal as $f(s|g,b)$. Using Bayesian updating, the firm's posterior belief about a candidate's productivity, given the signal $s$, group $g$, and bias choice $b$, is:
\begin{equation}
\mathbb{E}[\theta|s, g, b] = \frac{\sigma_\theta^2(s - b \cdot \mathbbm{1}_{g=1}) + \sigma_\varepsilon^2(b)\mu}{\sigma_\theta^2 + \sigma_\varepsilon^2(b)}
\end{equation}

\begin{longtable}{@{}ll@{}}
\caption{Model Parameters and Notation}
\label{tab:params}\\
\toprule
\textbf{Parameter} & \textbf{Description} \\ \midrule
\endfirsthead
\toprule
\textbf{Parameter} & \textbf{Description} \\ \midrule
\endhead
\bottomrule
\endfoot
$\theta$ & True candidate productivity, $\theta \sim N(\mu, \sigma_\theta^2)$ \\
$s_g$ & Algorithmic signal of productivity for group $g$ \\
$g \in \{0,1\}$ & Candidate group membership (observed) \\
$\pi_g$ & Proportion of group $g$ in the population, with $\pi_0+\pi_1=1$ \\
$b$ & Firm's choice of bias level, $b \in [0, b_{max}]$ \\
$t$ & Hiring threshold chosen by the firm \\
$\sigma_\varepsilon^2(b)$ & Variance of the algorithm's signal noise, a function of bias \\
$\eta_g$ & Exogenous group-specific measurement error, $\eta_g \sim N(0, \sigma_{\eta,g}^2)$ \\
$\sigma_0^2$ & Baseline signal noise when $b=b_{max}$ \\
$\kappa$ & Technological coupling parameter ($\kappa>0$) \\
$V(b)$ & The firm's value function, maximized at $b^*$ \\
$SWF(b)$ & Social welfare function \\
$E(b)$ & External costs of bias \\
\end{longtable}


\subsection{The precision-bias trade-off}
The model's central mechanism is a trade-off between signal precision and bias. This trade-off is at its core a natural result of how fairness constraints are currently implemented in machine learning systems. As established in the statistical learning literature, imposing a fairness constraint acts as a regularizer, often increasing estimator variance \citep{Kamishima2012, Wick2019}. This causes a dilemma: algorithms can be made fairer, but only at the cost of reduced accuracy.

This intuition is straightforward for economists familiar with constrained optimization. Fairness interventions in machine learning generally work through one of two channels. First, they can constrain the model to ignore certain correlations or features that are predictive but correlated with group membership, thus reducing the model's overall information set and subsequent precision. Second, they might use post-processing outputs to equalize outcomes across groups, the equivalent to intentionally adding noise or ``garbling" the signal in the spirit of Bayesian persuasion. In either case, enforcing fairness places some quantitative constraint on the optimization problem, moving the solution away from the unconstrained, accuracy-maximizing estimator and increasing the resulting prediction error.

Consider an algorithm predicting job performance. It might find that a feature like `captain of a high school sports team' is highly predictive. If this feature is also heavily correlated with one gender, a fairness constraint would likely force the algorithm to either ignore this feature or reduce its weight. In doing so, the algorithm discards predictive information, making its overall signal noisier (i.e., having higher variance) in exchange for generating more fair, or `debiased' predictions.

This relationship has been documented empirically over several machine learning applications. Studies in hiring algorithms, credit scoring, criminal justice risk assessment, and medical diagnosis consistently show that fairness constraints reduce predictive performance \citep{Kleinberg2017, Chouldechova2017}. The magnitude of this trade-off can vary by domain and algorithm type, but its existence is consistent across contexts.

We formalize this relationship with a functional form motivated by a linear approximation of this constraint.

\begin{definition}[Precision-bias trade-off]
The variance of the signal noise is given by:
\begin{equation}
\sigma_\varepsilon^2(b) = \sigma_0^2 + \kappa(b_{max} - b) \quad \text{for } b \in [0, b_{max}]
\end{equation}
\end{definition}

Here, $b=b_{max}$ describes the most precise but most biased signal, while $b=0$ represents a ``fair" signal with the highest noise. The parameter $\kappa > 0$ describes the steepness of this trade-off, where higher values of $\kappa$ tell that fairness comes at a greater cost to accuracy.\footnote{We use a linear form for the precision-bias trade-off for analytical tractability, which allows for a closed-form analysis. That said, our main result (that a profit-maximizing firm chooses a strictly positive level of bias ($b^*>0$)) is more general. The result holds for any trade-off function $\sigma_\varepsilon^2(b)$ that is monotonically decreasing and differentiable at $b=0$. As long as there is any marginal gain in signal precision from accepting a small amount of bias (i.e., $d\sigma_\varepsilon^2/db|_{b=0} < 0$), the firm will move away from the perfectly fair $b=0$ point.}

\subsection{Firm's problem}
The firm chooses bias $b$ and a hiring threshold $t$ to maximize the expected productivity of its hired workforce. This objective is standard in contexts with fixed hiring quotas or where talent maximization is the primary goal, and wages are fixed or separable. The firm's value is given by:
\begin{equation}
\max_{b, t} \ \mathbb{E}[U(b,t)] = \sum_{g \in \{0,1\}} \pi_g \int_t^\infty \mathbb{E}[\theta | s, g, b] f(s|g, b) ds
\end{equation}
The components of this problem, including the signal distribution $f(s|g,b)$ and the posterior expectation $\mathbb{E}[\theta|s,g,b]$, are formally defined in Appendix A.1.