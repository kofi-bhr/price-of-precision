\section{Conclusion}

This paper introduces a theoretical means of understanding the persistence of algorithmic bias: preserving biased signals for their informational value. Different from taste-based and statistical discrimination, this ``informational discrimination" emerges solely from the design of the technology available to us.

Our key insight is that firms can rationally choose biased algorithms because biased signals come with information that improves prediction accuracy. This sets up a dilemma between fairness and efficiency that one cannot resolve with preferences or market forces alone.

The model generates testable predictions about when and where algorithmic bias should be most common, and suggests that policies must address the foundational technological constraints rather than mandating fair outcomes.

Ultimately, the findings tell us that the path to algorithmic fairness is in fundamentally improving the underlying technology behind the outcomes, as opposed to imposing mandates on the outcomes themselves. By investing in research that `flattens the frontier' (yielding better fairness without sacrificing accuracy), we can build a space where firms' private incentives line up with the social goal of equity.