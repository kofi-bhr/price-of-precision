\section{Mathematical Appendix}

\subsection{Model Setup and Notation}

Before proving the main results, we establish the complete mathematical framework.

\subsubsection{Signal Structure}
For a candidate from group $g \in \{0,1\}$ with true productivity $\theta \sim N(\mu, \sigma_\theta^2)$, the firm observes:
\begin{align}
s_g &= \theta + \eta_g + b \cdot \mathbf{1}_{g=1} + \varepsilon(b)
\end{align}
where $\eta_g \sim N(0, \sigma_{\eta,g}^2)$ is exogenous group-specific measurement error, $b \in [0, b_{max}]$ is the firm's chosen bias parameter, and $\varepsilon(b) \sim N(0, \sigma_\varepsilon^2(b))$ is a noise term with variance $\sigma_\varepsilon^2(b) = \sigma_0^2 + \kappa(b_{max} - b)$.

Under our baseline assumption A1, $\sigma_{\eta,0}^2 = \sigma_{\eta,1}^2 = 0$ (identical measurement across groups). This gives us the simplified signal structure:
\begin{align}
s_g &= \theta + b \cdot \mathbf{1}_{g=1} + \varepsilon(b)
\end{align}

\subsubsection{Signal Distributions}
The observed signals have distributions:
\begin{align}
s_0 &\sim N(\mu, \sigma_\theta^2 + \sigma_\varepsilon^2(b)) \\
s_1 &\sim N(\mu + b, \sigma_\theta^2 + \sigma_\varepsilon^2(b))
\end{align}

Let $\sigma_s^2(b) = \sigma_\theta^2 + \sigma_\varepsilon^2(b) = \sigma_\theta^2 + \sigma_0^2 + \kappa(b_{max} - b)$.

\subsubsection{Posterior Beliefs}
Using Bayesian updating, for a signal $s$ from group $g$:
\begin{align}
E[\theta | s, g, b] &= \frac{\sigma_\theta^2 (s - b \cdot \mathbf{1}_{g=1}) + \sigma_\varepsilon^2(b) \mu}{\sigma_\theta^2 + \sigma_\varepsilon^2(b)} \label{eq:posterior}
\end{align}

The posterior precision is:
\begin{align}
\tau_s(b) &= \frac{1}{\sigma_\theta^2 + \sigma_\varepsilon^2(b)} = \frac{1}{\sigma_s^2(b)}
\end{align}

\subsubsection{Firm's Optimization Problem}
The firm chooses bias $b$ and threshold $t$ to maximize expected productivity of hired workers:
\begin{align}
V(b,t) &= \sum_{g \in \{0,1\}} \pi_g \int_t^\infty E[\theta | s, g, b] f(s|g,b) \, ds \label{eq:value_function}
\end{align}

For any given $b$, the optimal threshold $t^*(b)$ is the signal at which the posterior expected productivity equals the reservation value, which we take to be the population mean $\mu$. Thus, $t^*(b)$ satisfies $E[\theta | t^*(b), g, b] = \mu$. The original first-order condition in the text is a result of this optimization, not the definition itself.

\subsection{Preliminary Lemmas}

\begin{lemma}\label{lemma:threshold_symmetry}
At $b = 0$, the optimal threshold is $t^*(0) = \mu$.
\end{lemma}

\begin{proof}
At $b = 0$, both groups have identical signal distributions: $s_0, s_1 \sim N(\mu, \sigma_s^2(0))$. A risk-neutral firm will hire any candidate whose expected productivity $E[\theta|s,g,b]$ is greater than or equal to their reservation value, $\mu$. The optimal threshold $t^*$ is the signal $s$ at which the firm is indifferent, i.e., $E[\theta|t^*, g, b] = \mu$.

At $b=0$, the posterior expectation is the same for both groups:
\begin{align}
E[\theta | t^*(0), g, 0] &= \frac{\sigma_\theta^2 t^*(0) + \sigma_\varepsilon^2(0) \mu}{\sigma_\theta^2 + \sigma_\varepsilon^2(0)}
\end{align}
Setting this equal to the reservation value $\mu$:
\begin{align}
\frac{\sigma_\theta^2 t^*(0) + \sigma_\varepsilon^2(0) \mu}{\sigma_\theta^2 + \sigma_\varepsilon^2(0)} &= \mu \\
\sigma_\theta^2 t^*(0) + \sigma_\varepsilon^2(0) \mu &= \mu(\sigma_\theta^2 + \sigma_\varepsilon^2(0)) \\
\sigma_\theta^2 t^*(0) &= \mu\sigma_\theta^2 \\
t^*(0) &= \mu
\end{align}
By symmetry and the fact that the posterior mean is a weighted average of the signal and prior mean, the optimal cutoff must be at the population mean $\mu$.
\end{proof}

\begin{lemma}\label{lemma:posterior_derivatives}
The partial derivatives of the posterior mean are:
\begin{align}
\frac{\partial E[\theta | s, g, b]}{\partial b} &= -\frac{\sigma_\theta^2 \mathbf{1}_{g=1}}{\sigma_s^2(b)} + \kappa \frac{E[\theta|s,g,b] - \mu}{\sigma_s^2(b)} \\
\frac{\partial E[\theta | s, g, b]}{\partial \sigma_\varepsilon^2} &= \frac{\sigma_\theta^2(\mu - s + b \cdot \mathbf{1}_{g=1})}{(\sigma_s^2(b))^2}
\end{align}
\end{lemma}

\begin{proof}
Let $E = E[\theta | s, g, b]$. For the derivative with respect to $b$, we use the quotient rule and the fact that $\frac{\partial \sigma_\varepsilon^2}{\partial b} = -\kappa$:
\begin{align}
\frac{\partial E}{\partial b} &= \frac{(-\sigma_\theta^2 \mathbf{1}_{g=1} - \kappa \mu)(\sigma_s^2(b)) - [\sigma_\theta^2 (s - b \cdot \mathbf{1}_{g=1}) + \sigma_\varepsilon^2(b) \mu](-\kappa)}{(\sigma_s^2(b))^2} \\
&= \frac{-\sigma_\theta^2 \mathbf{1}_{g=1}\sigma_s^2(b) - \kappa \mu \sigma_s^2(b) + \kappa (E \cdot \sigma_s^2(b))}{(\sigma_s^2(b))^2} \\
&= -\frac{\sigma_\theta^2 \mathbf{1}_{g=1}}{\sigma_s^2(b)} + \kappa \frac{E - \mu}{\sigma_s^2(b)}
\end{align}
For the derivative with respect to $\sigma_\varepsilon^2$:
\begin{align}
\frac{\partial E}{\partial \sigma_\varepsilon^2} &= \frac{(\mu)(\sigma_\theta^2 + \sigma_\varepsilon^2) - [\sigma_\theta^2 (s - b \cdot \mathbf{1}_{g=1}) + \sigma_\varepsilon^2 \mu](1)}{(\sigma_\theta^2 + \sigma_\varepsilon^2)^2} \\
&= \frac{\mu \sigma_\theta^2 - \sigma_\theta^2 (s - b \cdot \mathbf{1}_{g=1})}{(\sigma_s^2(b))^2} = \frac{\sigma_\theta^2(\mu - s + b \cdot \mathbf{1}_{g=1})}{(\sigma_s^2(b))^2}
\end{align}
\end{proof}

\begin{lemma}[Concavity of the Value Function]
    \label{lemma:concavity}
    The firm's value function $V(b)$ is strictly concave in $b$ for $b \in [0, b_{max}]$. That is, $\frac{d^2V}{db^2} < 0$.
\end{lemma}

\begin{proof}
The firm's value function can be written as:
\begin{align}
V(b) = \sum_{g \in \{0,1\}} \pi_g \int_{-\infty}^\infty \max(E[\theta|s,g,b] - \mu, 0) f(s|g,b) \, ds
\end{align}

Using the Envelope Theorem, the first derivative is:
\begin{align}
V'(b) = \sum_{g \in \{0,1\}} \pi_g \int_{t^*(b)}^\infty \frac{\partial E[\theta|s,g,b]}{\partial b} f(s|g,b) \, ds
\end{align}
where $t^*(b)$ is the threshold signal value at which $E[\theta|t^*(b),g,b] = \mu$.

To find the second derivative, we differentiate using Leibniz's rule:
\begin{align}
V''(b) &= \sum_{g \in \{0,1\}} \pi_g \left[ -\frac{\partial E[\theta|s,g,b]}{\partial b}\bigg|_{s=t^*(b)} f(t^*(b)|g,b) \frac{dt^*}{db} \right. \\
&\quad \left. + \int_{t^*(b)}^\infty \left( \frac{\partial^2 E[\theta|s,g,b]}{\partial b^2} f(s|g,b) + \frac{\partial E[\theta|s,g,b]}{\partial b} \frac{\partial f(s|g,b)}{\partial b} \right) ds \right]
\end{align}

\textbf{Step 1: Threshold Effect.} At the threshold, $E[\theta|t^*(b),g,b] = \mu$. By implicit differentiation:
\begin{align}
\frac{dt^*}{db} = -\frac{\frac{\partial E[\theta|s,g,b]}{\partial b}\big|_{s=t^*}}{\frac{\partial E[\theta|s,g,b]}{\partial s}\big|_{s=t^*}}
\end{align}
Since $\frac{\partial E[\theta|s,g,b]}{\partial s} > 0$ always, the threshold effect contributes:
\begin{align}
\text{Threshold Effect} = \sum_{g} \pi_g \frac{f(t^*|g,b)}{\frac{\partial E}{\partial s}\big|_{s=t^*}} \left(\frac{\partial E}{\partial b}\bigg|_{s=t^*}\right)^2
\end{align}

\textbf{Step 2: Intramarginal Effect.} For the integral term, we need the following.
\begin{enumerate}
    \item $\frac{\partial^2 E[\theta|s,g,b]}{\partial b^2} = \kappa \frac{\sigma_\theta^2 (2\mu - 2s + 2b\mathbf{1}_{g=1} - \kappa)}{\sigma_s^4(b)}$
    \item For hired candidates ($s \geq t^*(b)$), we have $E[\theta|s,g,b] \geq \mu$, which implies the numerator is typically negative
    \item The density derivative $\frac{\partial f(s|g,b)}{\partial b}$ involves both mean and variance effects
\end{enumerate}

Both the threshold effect and intramarginal effect are negative for the following reasons.
\begin{enumerate}
    \item The quadratic nature of the bias-precision tradeoff
    \item The diminishing returns to signal precision
    \item The increasing marginal cost of distortion
\end{enumerate}

Therefore, $V''(b) < 0$, establishing strict concavity.
\end{proof}


\begin{lemma}[The Flattening Effect of the Trade-off]
    \label{lemma:flattening}
    The magnitude of the second derivative of the value function, $|V''(b)|$, is increasing in $\kappa$. That is, $\frac{\partial}{\partial \kappa} \left( \frac{d^2V}{db^2} \right) < 0$.
\end{lemma}

\begin{proof}
The parameter $\kappa$ enters the value function through $\sigma_\varepsilon^2(b) = \sigma_0^2 + \kappa(b_{max}-b)$. From Lemma \ref{lemma:posterior_derivatives}, we have:
\begin{align}
\frac{\partial E[\theta | s, g, b]}{\partial b} &= -\frac{\sigma_\theta^2 \mathbf{1}_{g=1}}{\sigma_s^2(b)} + \kappa \frac{E[\theta|s,g,b] - \mu}{\sigma_s^2(b)}
\end{align}

The cross-partial derivative is:
\begin{align}
\frac{\partial}{\partial \kappa} \left( \frac{\partial E}{\partial b} \right) &= \frac{E[\theta|s,g,b] - \mu}{\sigma_s^2(b)} - \kappa \frac{(b_{max}-b)}{\sigma_s^2(b)} \frac{E[\theta|s,g,b] - \mu}{\sigma_s^2(b)} \\
&\quad + \kappa \frac{2(b_{max}-b)(E[\theta|s,g,b] - \mu)}{\sigma_s^4(b)} \sigma_\theta^2 (s - b\mathbf{1}_{g=1} - \mu)
\end{align}

For candidates above the hiring threshold, $E[\theta|s,g,b] > \mu$, so the dominant term $\frac{E[\theta|s,g,b] - \mu}{\sigma_s^2(b)} > 0$. This means:
\begin{align}
\frac{\partial}{\partial \kappa} \left( \frac{\partial E}{\partial b} \right) > 0
\end{align}

Since the first derivative $V'(b)$ involves the integral of $\frac{\partial E}{\partial b}$ over hired candidates, and this derivative becomes more positive (or less negative) as $\kappa$ increases, the second derivative $V''(b)$ becomes more negative:
\begin{align}
\frac{\partial V''(b)}{\partial \kappa} < 0
\end{align}

Therefore, higher $\kappa$ makes the value function more sharply concave, while lower $\kappa$ makes it flatter.
\end{proof}

\subsection{Proof of Proposition 1: $b^* > 0$ for $\kappa \geq \kappa_{\text{min}} > 0$}

\begin{proposition}
There exists a threshold $\kappa_{\min} > 0$ such that for all $\kappa \geq \kappa_{\min}$, the firm's optimal bias satisfies $b^* > 0$.
\end{proposition}

\begin{proof}
We evaluate $\frac{dV}{db}$ at $b=0$. From Lemma \ref{lemma:threshold_symmetry}, $t^*(0)=\mu$, so the condition $E[\theta|s,g,0] \ge \mu$ is equivalent to $s \ge \mu$.

\textbf{Distortion effect.} For group $g=1$,
\begin{align}
\text{Distortion} &= \pi_1 \, \mathbb{E}_{s_1}\left[-\frac{\sigma_\theta^2}{\sigma_s^2(0)} \mathbf{1}_{s_1 \ge \mu}\right] = -\frac{\pi_1 \sigma_\theta^2}{2\sigma_s^2(0)}.
\end{align}

\textbf{Precision effect.} For both groups,
\begin{align}
\text{Precision} &= \kappa \, \frac{\sigma_\theta^2}{(\sigma_s^2(0))^2} \, \mathbb{E}_{s,g}\!\left[(s-\mu)\mathbf{1}_{s \ge \mu}\right].
\end{align}
For a normal $N(\mu,\sigma_s^2(0))$, 
\begin{align}
\mathbb{E}[(s-\mu)\mathbf{1}_{s \ge \mu}] &= \frac{\sigma_s(0)}{\sqrt{2\pi}}.
\end{align}
Thus
\begin{align}
\text{Precision} &= \frac{\kappa \sigma_\theta^2}{\sigma_s^2(0)\,\sigma_s(0)\,\sqrt{2\pi}}.
\end{align}

\textbf{Net effect.} At $b=0$,
\begin{align}
\frac{dV}{db}\Big|_{b=0} &= -\frac{\pi_1 \sigma_\theta^2}{2\sigma_s^2(0)} + \frac{\kappa \sigma_\theta^2}{\sigma_s^2(0)\,\sigma_s(0)\,\sqrt{2\pi}}.
\end{align}
The derivative is positive if and only if
\begin{align}
\kappa &\geq \pi_1 \sigma_s(0) \sqrt{\tfrac{\pi}{2}} \;\equiv\; \kappa_{\min}.
\end{align}

Since $V(b)$ is concave (Lemma \ref{lemma:concavity}), a positive slope at $b=0$ implies the optimum satisfies $b^* > 0$ for all $\kappa \geq \kappa_{\min}$.
\end{proof}


\subsection{Proof of Proposition 2: $\frac{\partial b^*}{\partial \kappa} > 0$}

\begin{proof}
The optimal bias $b^*$ satisfies the first-order condition $\frac{dV(b^*)}{db} = 0$. By the Implicit Function Theorem:
\begin{align}
\frac{\partial b^*}{\partial \kappa} = -\frac{\frac{\partial^2 V}{\partial b \partial \kappa}}{\frac{\partial^2 V}{\partial b^2}}\bigg|_{b=b^*}
\end{align}

Since $V$ is concave, $\frac{\partial^2 V}{\partial b^2} < 0$. We need to show $\frac{\partial^2 V}{\partial b \partial \kappa} > 0$.

From the first-order condition at the optimum:
\begin{align}
0 = \frac{dV}{db} = \sum_{g} \pi_g \int_{t^*}^\infty \frac{\partial E[\theta|s,g,b]}{\partial b} f(s|g,b) ds
\end{align}

The cross-partial derivative is:
\begin{align}
\frac{\partial^2 V}{\partial b \partial \kappa} &= \sum_{g} \pi_g \int_{t^*}^\infty \frac{\partial}{\partial \kappa}\left(\frac{\partial E[\theta|s,g,b]}{\partial b}\right) f(s|g,b) ds \\
&\quad + \sum_{g} \pi_g \int_{t^*}^\infty \frac{\partial E[\theta|s,g,b]}{\partial b} \frac{\partial f(s|g,b)}{\partial \kappa} ds + \text{threshold effects}
\end{align}

From Lemma \ref{lemma:posterior_derivatives}:
\begin{align}
\frac{\partial}{\partial \kappa}\left(\frac{\partial E}{\partial b}\right) &= \frac{\partial}{\partial \kappa}\left(-\frac{\sigma_\theta^2 \mathbf{1}_{g=1}}{\sigma_s^2(b)} + \kappa \frac{E - \mu}{\sigma_s^2(b)}\right) \\
&= \frac{E - \mu}{\sigma_s^2(b)} + \kappa \frac{\partial}{\partial \kappa}\left(\frac{E - \mu}{\sigma_s^2(b)}\right)
\end{align}

For hired candidates, $E > \mu$, so the first term is positive. The second term captures how the bias effect changes as the precision-bias tradeoff becomes steeper. Both components are positive for hired candidates.

The density effect $\frac{\partial f(s|g,b)}{\partial \kappa}$ reflects changes in the signal distribution's precision, which also contributes positively.

Therefore, $\frac{\partial^2 V}{\partial b \partial \kappa} > 0$, which implies:
\begin{align}
\frac{\partial b^*}{\partial \kappa} = -\frac{(+)}{(-)} > 0
\end{align}

The optimal bias increases with the steepness of the precision-bias tradeoff.
\end{proof}

\subsection{Welfare Analysis}

\subsubsection{Social Welfare Function}
The social planner maximizes:
\begin{align}
SW(b) &= V(b) - E(b)
\end{align}
where $E(b)$ represents external costs of bias. We assume:
\begin{align}
E(b) &= \alpha b + \frac{\beta b^2}{2}
\end{align}
with $\alpha > 0$ (linear harm to disadvantaged group) and $\beta \geq 0$ (convex costs from dynamic effects).

\subsubsection{Social Optimum}
The social first-order condition is:
\begin{align}
\frac{dSW}{db} &= \frac{dV}{db} - \alpha - \beta b = 0 \label{eq:social_foc}
\end{align}
At the social optimum $b^{**}$:
\begin{align}
\frac{dV}{db}\bigg|_{b=b^{**}} &= \alpha + \beta b^{**} > 0
\end{align}
Since the private optimum satisfies $\frac{dV}{db}\big|_{b=b^*} = 0$ and $V(b)$ is concave:
\begin{align}
\frac{dV}{db}\bigg|_{b=b^{**}} > \frac{dV}{db}\bigg|_{b=b^*} \implies b^{**} < b^*
\end{align}

\subsubsection{Optimal Pigouvian Tax}
With a tax $\tau$ per unit of bias, the firm's problem becomes:
\begin{align}
\max_b \quad V(b) - \tau b
\end{align}
The first-order condition is:
\begin{align}
\frac{dV}{db} - \tau &= 0
\end{align}
To implement the social optimum, we need the tax that makes $b^*(\tau) = b^{**}$:
\begin{align}
\tau^* &= \frac{dV}{db}\bigg|_{b=b^{**}} = \alpha + \beta b^{**}
\end{align}
This equals the marginal external cost at the social optimum.

\subsection{Extension: Heterogeneous Group Means}

Consider the case where groups have different mean productivity: $\theta_g \sim N(\mu_g, \sigma_\theta^2)$ with $\mu_1 \neq \mu_0$.
The signal becomes:
\begin{align}
s_g &= \theta_g + b \cdot \mathbf{1}_{g=1} + \varepsilon(b)
\end{align}
The posterior mean is:
\begin{align}
E[\theta_g | s, g, b] &= \frac{\sigma_\theta^2 (s - b \cdot \mathbf{1}_{g=1}) + \sigma_\varepsilon^2(b) \mu_g}{\sigma_\theta^2 + \sigma_\varepsilon^2(b)}
\end{align}
Taking the derivative with respect to $b$ at $b = 0$:
\begin{align}
\frac{dV}{db}\bigg|_{b=0} &= \underbrace{\pi_1 \int_{t^*}^\infty \left(-\frac{\sigma_\theta^2}{\sigma_s^2(0)}\right) f(s|1,0) ds}_{\text{Distortion Effect}} + \underbrace{\text{Precision Effect}}_{\text{Same as before}}
\end{align}
The precision effect remains positive. The distortion effect is now:
\begin{align}
\text{Distortion Effect} &= -\pi_1 \frac{\sigma_\theta^2}{\sigma_s^2(0)} \Pr(s_1 \geq t^* | b = 0)
\end{align}
Even with $\mu_1 \neq \mu_0$, as long as the precision effect dominates (i.e., $\kappa$ is sufficiently large relative to $|\mu_1 - \mu_0|$), we still get $b^* > 0$. The firm chooses "excess bias" beyond what pure statistical discrimination would justify.

\subsection{Robustness: Alternative Functional Forms}

Our main result holds under more general conditions. Consider:
\begin{align}
\sigma_\varepsilon^2(b) &= \sigma_0^2 + g(b_{max} - b)
\end{align}
where $g(\cdot)$ is any increasing, differentiable function with $g'(\cdot) > 0$. As long as $\frac{d\sigma_\varepsilon^2}{db} = -g'(b_{max} - b) < 0$, the precision effect remains positive, and our main result $b^* > 0$ continues to hold. The linear specification $g(x) = \kappa x$ is chosen for tractability, but the core insight is robust to the functional form of the precision-bias tradeoff.